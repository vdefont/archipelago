{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addressed-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable, List, Tuple\n",
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attended-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from explainer import Archipelago\n",
    "from application_utils.text_utils import TextXformer\n",
    "from application_utils.text_utils_torch import BertWrapperTorch\n",
    "from viz.text import viz_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "material-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_PATH = \"gpt_model\"\n",
    "\n",
    "\n",
    "def download_models() -> None:\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "    tokenizer.save_pretrained(GPT2_PATH)\n",
    "    model.save_pretrained(GPT2_PATH)\n",
    "\n",
    "\n",
    "def load_model_from_file() -> Tuple[Any, Any]:\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt_model')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt_model')\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "treated-gnome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " store park local grocery beach mall gym library hospital school\n"
     ]
    }
   ],
   "source": [
    "# Do an example\n",
    "\n",
    "# download_models()\n",
    "tokenizer, model = load_model_from_file()\n",
    "sent = \"yesterday afternoon me and my son went to the\"\n",
    "\n",
    "tokens = tokenizer(sent).input_ids  # (sent_len,)\n",
    "preds = model(torch.LongTensor(tokens)).logits  # (sent_len, vocab_size)\n",
    "next_word_preds = preds[-1]\n",
    "next_word_tokens = torch.topk(next_word_preds, k=10).indices\n",
    "next_word_logits = torch.topk(next_word_preds, k=10).values\n",
    "next_words = tokenizer.decode(next_word_tokens)\n",
    "print(next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "marked-destiny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expected-olive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8505, 6432, 6672, 502, 290, 616, 3367, 1816, 284, 262],\n",
       " tensor([ 3650,  3952,  1957, 16918, 10481, 17374, 11550,  5888,  4436,  1524]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, next_word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suffering-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_next_token = 3650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charged-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('_').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "geographic-rapid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 62, 62, 62, 62, 62, 62, 62, 62, 62]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_token = tokenizer('_').input_ids\n",
    "baseline_token * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "purple-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTWrapperTorch:\n",
    "    def __init__(self, model: Any, device: str, merge_logits: bool = False) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, batch_ids: Iterable[Iterable[int]]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Input: A batch of examples, where each example is a list of tokens\n",
    "        - shape = (batch_size, sent_len)\n",
    "        Output: For each example, the logits for the likelihood of for all 50257 tokens to be the next word\n",
    "        - shape = (batch_size, 50257)\n",
    "        \"\"\"\n",
    "        preds = model(torch.LongTensor(batch_ids)).logits  # (batch_size, sent_len, vocab_size)\n",
    "        next_word_preds = preds[:, -1]  # (batch_size, vocab_size)\n",
    "        return next_word_preds.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "protective-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ids, baseline_ids = tokens, baseline_token * len(tokens)\n",
    "# output_indices = next_word_tokens.detach().numpy()\n",
    "output_indices = most_likely_next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "referenced-cherry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8505, 6432, 6672, 502, 290, 616, 3367, 1816, 284, 262],\n",
       " [62, 62, 62, 62, 62, 62, 62, 62, 62, 62],\n",
       " 3650)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids, baseline_ids, output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "discrete-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = GPTWrapperTorch(model=model, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "casual-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = TextXformer(text_ids, baseline_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moderate-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "apgo = Archipelago(\n",
    "    model_wrapper, \n",
    "    data_xformer=xf, \n",
    "    output_indices=output_indices,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controlling-tuner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_tuple (0,)\n",
      "preds [[ 2.369074   4.3052573  4.854746  ... -4.128131  -5.1946387  0.5706473]\n",
      " [ 2.5863144  4.1248903  3.4009182 ... -4.5984774 -5.177022   2.6482196]]\n",
      "c 0\n",
      "self.output_indices 3650\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "{(0,): -3.0886092}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-22cdb7ca1918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/archipelago/src/explainer.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, top_k, separate_effects)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m### 1: DETECT ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_effects\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mdetection_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pairwise_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0minter_strengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interactions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_effects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"main_effects\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/archipelago/src/explainer.py\u001b[0m in \u001b[0;36marchdetect\u001b[0;34m(self, get_main_effects, get_pairwise_effects, single_context, weights)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mget_main_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_main_effects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mget_pairwise_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_pairwise_effects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    158\u001b[0m         \u001b[0minter_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"interactions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/archipelago/src/explainer.py\u001b[0m in \u001b[0;36msearch_feature_sets\u001b[0;34m(self, context, insertion_target, get_interactions, get_main_effects, get_pairwise_effects)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         preds = self.batch_set_inference(\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0midv_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsertion_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[0midv_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/archipelago/src/explainer.py\u001b[0m in \u001b[0;36mbatch_set_inference\u001b[0;34m(self, set_indices, context, insertion_target, include_context)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self.output_indices\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_tuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minclude_context\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mcontext_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: {(0,): -3.0886092}"
     ]
    }
   ],
   "source": [
    "explanation = apgo.explain(top_k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-replica",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
